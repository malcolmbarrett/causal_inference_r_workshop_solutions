---
title: "Continuous exposures and g-computation"
format: html
---

```{r}
#| label: setup
library(tidyverse)
library(broom)
library(touringplans)
library(splines)
```

For this set of exercises, we'll use g-computation to calculate a causal effect for continuous exposures.

In the touringplans data set, we have information about the posted waiting times for rides. We also have a limited amount of data on the observed, actual times. The question that we will consider is this: Do posted wait times (`avg_spostmin`) for the Seven Dwarves Mine Train at 8 am affect actual wait times (`avg_sactmin`) at 9 am? Here’s our DAG:

```{r}
#| echo: false
#| message: false
#| warning: false
library(ggdag)
library(ggokabeito)

coord_dag <- list(
  x = c(Season = -1, close = -1, weather = -2, extra = 0, x = 1, y = 2),
  y = c(Season = -1, close = 1, weather = 0, extra = 0, x = 0, y = 0)
)

labels <- c(
  extra = "Extra Magic Morning",
  x = "Average posted wait ",
  y = "Average acutal wait",
  Season = "Ticket Season",
  weather = "Historic high temperature",
  close = "Time park closed"
)

dagify(
  y ~ x + close + Season + weather + extra,
  x ~ weather + close + Season + extra,
  extra ~ weather + close + Season,
  coords = coord_dag,
  labels = labels,
  exposure = "x",
  outcome = "y"
) |>
  tidy_dagitty() |>
  node_status() |>
  ggplot(
    aes(x, y, xend = xend, yend = yend, color = status)
  ) +
  geom_dag_edges_arc(curvature = c(rep(0, 7), .2, 0, .2, .2, 0), edge_colour = "grey70") +
  geom_dag_point() +
  geom_dag_label_repel(
    aes(x, y, label = label),
    box.padding = 3.5, 
    inherit.aes = FALSE,
    max.overlaps = Inf, 
    family = "sans",
    seed = 1602,
    label.size = NA, 
    label.padding = 0.1,
    size = 14 / 3
  )  + 
  scale_color_okabe_ito(na.value = "grey90") +
  theme_dag() +
  theme(
    legend.position = "none",
    axis.text.x = element_text()
  ) +
  coord_cartesian(clip = "off") +
  scale_x_continuous(
    limits = c(-2.25, 2.25),
    breaks = c(-2, -1, 0, 1, 2),
    labels = c(
      "\n(one year ago)",
      "\n(6 months ago)",
      "\n(3 months ago)",
      "8am-9am\n(Today)",
      "9am-10am\n(Today)"
    )
  )
```

First, let’s wrangle our data to address our question: do posted wait times at 8 affect actual weight times at 9? We’ll join the baseline data (all covariates and posted wait time at 8) with the outcome (average actual time). We also have a lot of missingness for `avg_sactmin`, so we’ll drop unobserved values for now.

You don't need to update any code here, so just run this.

```{r}
eight <- seven_dwarfs_train_2018 |>
  filter(hour == 8) |>
  select(-avg_sactmin)

nine <- seven_dwarfs_train_2018 |>
  filter(hour == 9) |>
  select(date, avg_sactmin)

wait_times <- eight |>
  left_join(nine, by = "date") |>
  drop_na(avg_sactmin)
```

# Your Turn 1

For the parametric G-formula, we'll use a single model to fit a causal model of Posted Waiting Times (`avg_spostmin`) on Actual Waiting Times (`avg_sactmin`) where we  include all covariates, much as we normally fit regression models. However, instead of interpreting the coefficients, we'll calculate the estimate by predicting on cloned data sets.

Two additional differences in our model: we'll use a natural cubic spline on the exposure, `avg_spostmin`, using `ns()` from the splines package, and we'll include an interaction term between `avg_spostmin` and `extra_magic_mornin g`. These complicate the interpretation of the coefficient of the model in normal regression but have virtually no downside (as long as we have a reasonable sample size) in g-computation, because we still get an easily interpretable result.

First, let's fit the model. 

1.Use `lm()` to create a model with the outcome, exposure, and confounders identified in the DAG. 
2. Save the model as `standardized_model`

```{r}
standardized_model <-  lm(
  avg_sactmin ~ ns(avg_spostmin, df = 5)*extra_magic_morning + weather_wdwhigh + wdw_ticket_season + close, 
  data = seven_dwarfs
)
```

# Your Turn 2

Now that we've fit a model, we need to clone our data set. To do this, we'll simply mutate it so that in one set, all participants have `avg_spostmin` set to 30 minutes and in another, all participants have `avg_spostmin` set to 60 minutes. 

1. Create the cloned data sets, called `thirty` and `sixty`.
2. For both data sets, use `standardized_model` and `augment()` to get the predicted values. Use the `newdata` argument in `augment()` with the relevant cloned data set. Then, select only the fitted value. Rename `.fitted` to either `thirty_posted_minutes` or `sixty_posted_minutes` (use the pattern `select(new_name = old_name)`).
3. Save the predicted data sets as`predicted_thirty` and `predicted_sixty`.

```{r}
thirty <- seven_dwarfs |>
  mutate(avg_spostmin = 30)

sixty <- seven_dwarfs |>
  mutate(avg_spostmin = 60)

predicted_thirty <- standardized_model |>
  augment(newdata = thirty) |>
  select(thirty_posted_minutes = .fitted)

predicted_sixty <- standardized_model |>
  augment(newdata = sixty) |>
  select(sixty_posted_minutes = .fitted)
```

# Your Turn 3

Finally, we'll get the mean differences between the values. 

1. Bind `predicted_thirty` and  `predicted_sixty` using `bind_cols()`
2. Summarize the predicted values to create three new variables: `mean_thirty`, `mean_sixty`, and `difference`. The first two should be the means of `thirty_posted_minutes` and `sixty_posted_minutes`. `difference` should be `mean_sixty` minus `mean_thirty`.

```{r}
bind_cols(predicted_thirty, predicted_sixty) |>
  summarize(
    mean_thirty = mean(thirty_posted_minutes),
    mean_sixty = mean(sixty_posted_minutes),
    difference = mean_sixty - mean_thirty
  )
```

That's it! `difference` is our effect estimate, marginalized over the spline terms, interaction effects, and confounders.

## Stretch goal: Boostrapped intervals

Like propensity-based models, we need to do a little more work to get correct standard errors and confidence intervals. In this stretch goal, use rsample to bootstrap the estimates we got from the G-computation model.

Remember, you need to bootstrap the entire modeling process, including the regression model, cloning the data sets, and calculating the effects.

```{r}
set.seed(1234)
library(rsample)

fit_gcomp <- function(split, ...) { 
  .df <- analysis(split) 
  
  # fit outcome model. remember to model using `.df` instead of `seven_dwarfs`
  standardized_model <-  lm(
    avg_sactmin ~ ns(avg_spostmin, df = 5)*extra_magic_morning + weather_wdwhigh + wdw_ticket_season + close, 
    data = .df
  )
  
  # clone datasets. remember to clone `.df` instead of `seven_dwarfs`
  thirty <- .df |>
    mutate(avg_spostmin = 30)
  
  sixty <- .df |>
    mutate(avg_spostmin = 60)
  
  # predict actual wait time for each cloned dataset
  predicted_thirty <- standardized_model |>
    augment(newdata = thirty) |>
    select(thirty_posted_minutes = .fitted)
  
  predicted_sixty <- standardized_model |>
    augment(newdata = sixty) |>
    select(sixty_posted_minutes = .fitted)
  
  # calculate ATE
  bind_cols(predicted_thirty, predicted_sixty) |>
    summarize(
      mean_thirty = mean(thirty_posted_minutes),
      mean_sixty = mean(sixty_posted_minutes),
      difference = mean_sixty - mean_thirty
    ) |>
    # rsample expects a `term` and `estimate` column
    pivot_longer(everything(), names_to = "term", values_to = "estimate")
}

gcomp_results <- bootstraps(seven_dwarfs, 1000, apparent = TRUE) |>
  mutate(results = map(splits, fit_gcomp))

# using bias-corrected confidence intervals
boot_estimate <- int_bca(gcomp_results, results, .fn = fit_gcomp)

boot_estimate
```

***

# Take aways

* To fit the parametric G-formula, fit a standardized model with all covariates. Then, use cloned data sets with values set to each level of the exposure you want to study. 
* Use the model to predict the values for that level of the exposure and compute the effect estimate you want
